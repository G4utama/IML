\section{Neural Networks}

\begin{mdframed}
    \textbf{Neural Networks:} model inspired by the structure and function of biological neural networks in animal brains.
\end{mdframed}

\subsection{Architecture}
Architecture in the context of neural networks refers to the pattern developed to connect different neurons together.  Typically, neurons are organized by levels (layers).  In this course, only fully connected architectures are addressed, that is, where all neurons in one layer are connected to those in the next layer.
\begin{center}
    \begin{tabular}{c}
        \\ \includegraphics[width=0.35\textwidth]{images/NeuralNetworks1.png} \\ \\
    \end{tabular}
\end{center}
In this example a 2 layer neural network is schematized, one of which is hidden, for the nomenclature we do not consider the input layer since it is not affected by the parameters.  When going to establish the architecture of a neural network one goes to establish the number of parameters to be learned.
\begin{center}
    \begin{tabular}{c}
        \\ \includegraphics[width=0.6\textwidth]{images/NeuralNetworks2.png} \\ \\
    \end{tabular}
\end{center}
The total number of parameters is given by the sum of:
\begin{itemize}
    \item $3$ (\# neurons \textcolor{red}{input layer}) $\times 4$ (\# neurons \textcolor{blue}{hidden layer 1})
    \item $4$ (\# neurons \textcolor{blue}{hidden layer 1}) $\times 4$ (\# neurons \textcolor{blue}{hidden layer 2})
    \item $4$ (\# neurons \textcolor{blue}{hidden layer 2}) $\times 1$ (\# neurons \textcolor{green}{output layer})
    \item $4$ (bias \textcolor{blue}{hidden layer 1})
    \item $4$ (bias \textcolor{blue}{hidden layer 2})
    \item $1$ (bias \textcolor{green}{output layer})
\end{itemize}

\subsection{Feed-Forward Computation}
\begin{mdframed}
    \textbf{Feed-Forward:} neural networks in which the outputs of one layer become the inputs of the next layer.   Each output will be the weighted sum of the inputs of the previous layer processed by the activation function, and this will be repeated until the final output is reached.
\end{mdframed}
\begin{center}
    \begin{tabular}{c}
        \\ \includegraphics[width=0.5\textwidth]{images/NeuralNetworks3.png} \\ \\
    \end{tabular}
\end{center}
Node $a_1^{(2)}$ takes as input the three outputs of the first layer and through the nonlinear function $f$ calculates the linear combination of the three inputs plus the bias:
\begin{equation} \tag*{}
    a_1^{(2)} = f(\theta_{10}^(1)x_0 + \theta_{11}^(1)x_1 + \theta_{12}^(1)x_2 + \theta_{13}^(1)x_3)
\end{equation}
This is done for all nodes in the layer.
$\theta^{(j)}$ denotes the matrix of weights between layer $j$ and $j+1$.
The summation of all linear combinations of a layer is denoted by the letter $z_i^{(j)}$ where $j$ denotes the number of the layer, in this case $a_1^{(2)} = f(z_1^{(2)})$.
If we focus only on the last hidden layer we can see that what we do is the same as logistic regression in which the function $h_\theta(x) =f(\theta^{(2)}a^{(2)})$ and the input is no longer the features but is the $a^{(2)}$ learned from the network.
This makes us realize that all the previous hidden layers do is learn latent features from the data.





\newpage